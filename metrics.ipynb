{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# replace this with a Spark history log of your own or parameterize with Papermill!\n",
    "\n",
    "metrics_file = \"metrics/application_1601392010735_0030\"\n",
    "\n",
    "output_file = \"output.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = spark.read.json(metrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id, app_name = metrics.select(\"App ID\", \"App Name\").dropna().collect()[0]\n",
    "\n",
    "def with_appmeta(df):\n",
    "    return df.withColumn(\"Application ID\", F.lit(app_id)).withColumn(\"Application Name\", F.lit(app_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_dictify(df):\n",
    "    return [json.loads(row[0]) for row in df.selectExpr(\"to_json(*)\").collect()]\n",
    "\n",
    "def executor_info(df):\n",
    "    info = df.select(\"Executor Info\").dropna()\n",
    "    return collect_and_dictify(info)\n",
    "\n",
    "def plan_dicts(df):\n",
    "    return collect_and_dictify(df.select(\"sparkPlanInfo\").dropna())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "MetricNode = namedtuple(\"MetricNode\", \"plan_node accumulatorId metricType name\")\n",
    "PlanInfoNode = namedtuple(\"PlanInfoNode\", \"plan_node parent nodeName simpleString\")\n",
    "\n",
    "def nextid():\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield i\n",
    "        i = i + 1\n",
    "    \n",
    "node_ctr = nextid()\n",
    "\n",
    "def plan_dicts(df):\n",
    "    return collect_and_dictify(df.select(\"sparkPlanInfo\").dropna())\n",
    "\n",
    "def flatplan(dicts, parent=-1, plan_nodes=None, metric_nodes=None):\n",
    "    if plan_nodes is None:\n",
    "        plan_nodes = list()\n",
    "        \n",
    "    if metric_nodes is None:\n",
    "        metric_nodes = list()\n",
    "    \n",
    "    for pd in dicts:\n",
    "        pid = next(node_ctr)\n",
    "        for m in pd['metrics']:\n",
    "            metric_nodes.append(MetricNode(pid, m['accumulatorId'], m['metricType'], m['name']))\n",
    "        \n",
    "        plan_nodes.append(PlanInfoNode(pid, parent, pd['nodeName'], pd['simpleString']))\n",
    "        \n",
    "        flatplan(pd['children'], pid, plan_nodes, metric_nodes)\n",
    "    \n",
    "    return(plan_nodes, metric_nodes)\n",
    "\n",
    "def plan_dfs(df):\n",
    "    pn, mn = flatplan(plan_dicts(metrics))\n",
    "    \n",
    "    pndf = with_appmeta(spark.createDataFrame(data=pn))\n",
    "    mndf = with_appmeta(spark.createDataFrame(data=mn))\n",
    "    \n",
    "    return (pndf, mndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_nodes, accumulable_nodes = plan_dfs(metrics)\n",
    "pn, mn = flatplan(plan_dicts(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tasks_to_stages(df):\n",
    "    return df.where(F.col('Event') == 'SparkListenerTaskStart').select(F.col(\"Task Info.Task ID\").alias('Task ID'), 'Stage ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulables(df, noun='Task', extra_cols=[]):\n",
    "    mcol = '%s Info' % noun\n",
    "    idcol = '%s ID' % noun\n",
    "    \n",
    "    acc_cols = [F.col('Accumulable.%s' % s).alias('Metric %s' % s) for s in ['ID', 'Name', 'Value']]\n",
    "    obs = df.select(mcol, *extra_cols).select('%s.*' % mcol, *extra_cols)\n",
    "    cols = [F.col(elt) for elt in sorted(set(obs.columns) - set([idcol, 'Accumulables']))]\n",
    "    \n",
    "    return obs.select(\n",
    "        idcol, \n",
    "        F.explode('Accumulables').alias('Accumulable'), \n",
    "        *(cols)\n",
    "    ).select(\n",
    "        idcol, \n",
    "        *(cols + acc_cols)\n",
    "    ).withColumnRenamed(\"Metric ID\", \"accumulatorId\").withColumn(\"Metric Value\", F.col(\"Metric Value\").cast(\"float\"))\n",
    "\n",
    "def explicit_task_metrics(df, noun=\"Task\"):\n",
    "    def flatten_once(df):\n",
    "        import pyspark.sql.types as T\n",
    "\n",
    "        cols = []\n",
    "    \n",
    "        for column in df.schema:\n",
    "            if isinstance(column.dataType, T.StructType):\n",
    "                for subfield in column.dataType:\n",
    "                    cols.append(F.col(\"%s.%s\" % (column.name, subfield.name)).alias(\"%s %s\" % (column.name, subfield.name)))\n",
    "            elif isinstance(column.dataType, T.ArrayType):\n",
    "                pass\n",
    "            else:\n",
    "                cols.append(F.col(column.name))\n",
    "\n",
    "        return df.select(*cols)\n",
    "    \n",
    "    mcol = '%s Info' % noun\n",
    "    idcol = '%s ID' % noun\n",
    "    \n",
    "    obs = df.select('%s.*' % mcol, \"Stage ID\", F.col(\"Task Metrics.*\"), F.col(\"Task Executor Metrics.*\"))\n",
    "    cols = [F.col(elt) for elt in obs.columns if elt not in set([idcol, 'Accumulables'])]\n",
    "\n",
    "    flat_df = flatten_once(obs.select(idcol, *cols))\n",
    "    \n",
    "    common_split = flat_df.columns.index(\"Stage ID\") + 1\n",
    "    \n",
    "    common_columns = flat_df.columns[:common_split]\n",
    "    metric_columns = flat_df.columns[common_split:]\n",
    "    \n",
    "    result = flat_df.where(\n",
    "        F.lit(False)\n",
    "    ).select(\n",
    "        *(common_columns + \n",
    "          [\n",
    "              F.lit(None).alias(\"accumulatorID\"), \n",
    "              F.lit(None).alias(\"Metric Name\"), \n",
    "              F.lit(None).alias(\"Metric Value\")\n",
    "          ]\n",
    "         )\n",
    "    )\n",
    "    \n",
    "    for col in metric_columns:\n",
    "        result = result.union(\n",
    "            flat_df.select(\n",
    "                *(\n",
    "                    common_columns + \n",
    "                    [\n",
    "                        F.lit(None).alias(\"accumulatorID\"), \n",
    "                        F.lit(col).alias(\"Metric Name\"), \n",
    "                        F.col(col).alias(\"Metric Value\")\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def tidy_metrics(df, noun='Task', event=None, interesting_metrics=None, extra_cols=[]):\n",
    "    mcol = '%s Info' % noun\n",
    "    idcol = '%s ID' % noun\n",
    "    \n",
    "    if event is not None:\n",
    "        event_selector = (F.col('Event') == event)\n",
    "    else:\n",
    "        event_selector = F.lit(True)\n",
    "    \n",
    "    filtered = df.where(event_selector)\n",
    "    \n",
    "    return with_appmeta(accumulables(filtered, noun, extra_cols).unionByName(explicit_task_metrics(filtered)))\n",
    "\n",
    "def tidy_tasks(df, event='SparkListenerTaskEnd', interesting_metrics=None):\n",
    "    return tidy_metrics(df, 'Task', event=event, interesting_metrics=(interesting_metrics or F.lit(True)), extra_cols=[\"Stage ID\"])\n",
    "\n",
    "def tidy_stages(df, event='SparkListenerStageCompleted', interesting_metrics=None):\n",
    "    return tidy_metrics(df, 'Stage', event=event, interesting_metrics=F.lit(True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_metrics(df):\n",
    "    metric_columns = set([\"Metric Name\", \"Metric Value\", \"accumulatorId\", \"kind\", \"unit\"])\n",
    "    common_columns = set([\"Application ID\", \"Application Name\", \"Task ID\", \"Stage ID\"])\n",
    "    \n",
    "    metrics = df.select(*[col for col in df.columns if col in metric_columns or col in common_columns])\n",
    "    task_meta = df.drop(*metric_columns).distinct()\n",
    "    \n",
    "    return (metrics, task_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetricMeta = namedtuple('MetricMeta', 'MetricName kind unit')\n",
    "\n",
    "metric_metas = [\n",
    "    MetricMeta('GPU decode time', 'time', 'ms'),\n",
    "    MetricMeta('GPU time', 'time', 'ms'),\n",
    "    MetricMeta('avg hash probe bucket list iters', 'count', 'iterations'),\n",
    "    MetricMeta('buffer time', 'time', 'ms'),\n",
    "    MetricMeta('build side size', 'size', 'bytes'),\n",
    "    MetricMeta('build time', 'time', 'ms'),\n",
    "    MetricMeta('collect batch time', 'time', 'ms'),\n",
    "    MetricMeta('concat batch time', 'time', 'ms'),\n",
    "    MetricMeta('data size', 'size', 'bytes'),\n",
    "    MetricMeta('duration', 'time', 'ms'),\n",
    "    MetricMeta('fetch wait time', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.diskBytesSpilled', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.executorCpuTime', 'time', 'ns'),\n",
    "    MetricMeta('internal.metrics.executorDeserializeCpuTime', 'time', 'ns'),\n",
    "    MetricMeta('internal.metrics.executorDeserializeTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.executorRunTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.input.bytesRead', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.input.recordsRead', 'count', 'records'),\n",
    "    MetricMeta('internal.metrics.jvmGCTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.memoryBytesSpilled', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.output.bytesWritten', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.output.recordsWritten', 'count', 'records'),\n",
    "    MetricMeta('internal.metrics.peakExecutionMemory', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.resultSerializationTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.resultSize', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.fetchWaitTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.localBlocksFetched', 'count', 'blocks'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.localBytesRead', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.recordsRead', 'count', 'records'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.remoteBlocksFetched', 'count', 'blocks'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.remoteBytesRead', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.remoteBytesReadToDisk', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.write.bytesWritten', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.write.recordsWritten', 'count', 'records'),\n",
    "    MetricMeta('internal.metrics.shuffle.write.writeTime', 'time', 'ms'),\n",
    "    MetricMeta('join output rows', 'count', 'rows'),\n",
    "    MetricMeta('join time', 'time', 'ms'),\n",
    "    MetricMeta('local blocks read', 'count', 'blocks'),\n",
    "    MetricMeta('local bytes read', 'size', 'bytes'),\n",
    "    MetricMeta('number of input columnar batches', 'count', 'batches'),\n",
    "    MetricMeta('number of input rows', 'count', 'rows'),\n",
    "    MetricMeta('number of output columnar batches', 'count', 'batches'),\n",
    "    MetricMeta('number of output rows', 'count', 'rows'),\n",
    "    MetricMeta('peak device memory', 'size', 'bytes'),\n",
    "    MetricMeta('peak memory', 'size', 'bytes'),\n",
    "    MetricMeta('records read', 'count', 'records'),\n",
    "    MetricMeta('remote blocks read', 'count', 'blocks'),\n",
    "    MetricMeta('remote bytes read', 'size', 'bytes'),\n",
    "    MetricMeta('scan time', 'time', 'ms'),\n",
    "    MetricMeta('shuffle bytes written', 'size', 'bytes'),\n",
    "    MetricMeta('shuffle records written', 'count', 'records'),\n",
    "    MetricMeta('shuffle write time', 'time', 'ms'),\n",
    "    MetricMeta('spill size', 'size', 'bytes'),\n",
    "    MetricMeta('sort time', 'time', 'ms'),\n",
    "    MetricMeta('time in aggregation build', 'time', 'ms'),\n",
    "    MetricMeta('time in batch concat', 'time', 'ms'),\n",
    "    MetricMeta('time in compute agg', 'time', 'ms'),\n",
    "    MetricMeta('total time', 'time', 'ns'),\n",
    "    MetricMeta('write time', 'time', 'ms')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_meta = spark.createDataFrame(data=metric_metas)\n",
    "task_metrics = tidy_tasks(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_plans = with_appmeta(task_metrics.join(accumulable_nodes, \"accumulatorId\").join(plan_nodes, \"plan_node\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_plans.groupBy(\"nodeName\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.data_transformers.enable('json')\n",
    "\n",
    "def stage_and_task_charts(task_metrics_df, noun=\"Time\"):\n",
    "    \n",
    "    selection = alt.selection_multi(name=\"SelectorName\", fields=['Stage ID'])\n",
    "    stage_metrics_df = task_metrics_df.groupby(['Stage ID', 'Metric Name']).sum()\n",
    "    \n",
    "    stages = alt.Chart(\n",
    "        stage_metrics_df.reset_index()\n",
    "    ).mark_bar().encode(\n",
    "        x='Stage ID:N',\n",
    "        y=alt.Y('sum(Metric Value):Q', title=noun),\n",
    "        color='Metric Name:N',\n",
    "        tooltip=['Metric Name', 'Metric Value', 'Task ID']\n",
    "    ).add_selection(selection).interactive()\n",
    "    \n",
    "    tasks = alt.Chart(\n",
    "        task_metrics_df.reset_index()\n",
    "    ).mark_bar().encode(\n",
    "        x='Task ID:N',\n",
    "        y=alt.Y('sum(Metric Value):Q', title=noun),\n",
    "        color='Metric Name:N',\n",
    "        tooltip=['Metric Name', 'Metric Value', 'Task ID']\n",
    "    ).transform_filter(\n",
    "        selection\n",
    "    ).interactive()\n",
    "\n",
    "    return alt.vconcat(stages, tasks)\n",
    "\n",
    "def layered_stage_and_task_charts(task_layers, noun=\"Time\"):\n",
    "    \n",
    "    selection = alt.selection_multi(name=\"selector_SelectorName\", fields=['Stage ID'])\n",
    "    sdfs = [tdf.groupby(['Stage ID', 'Metric Name']).sum() for tdf in task_layers]\n",
    "    \n",
    "    stages = alt.layer(*[alt.Chart(\n",
    "        sdf.reset_index()\n",
    "    ).mark_bar().encode(\n",
    "        x='Stage ID:N',\n",
    "        y=alt.Y('sum(Metric Value):Q', title=noun),\n",
    "        color='Metric Name:N',\n",
    "        tooltip=['Metric Name', 'Metric Value', 'Task ID']\n",
    "    ) for sdf in sdfs]).add_selection(selection).interactive()\n",
    "    \n",
    "    tasks = alt.layer(*[alt.Chart(\n",
    "        tdf.reset_index()\n",
    "    ).mark_bar().encode(\n",
    "        x='Task ID:N',\n",
    "        y=alt.Y('sum(Metric Value):Q', title=noun),\n",
    "        color='Metric Name:N',\n",
    "        tooltip=['Metric Name', 'Metric Value', 'Task ID']\n",
    "    ).transform_filter(\n",
    "        selection\n",
    "    ) for tdf in task_layers]).interactive()\n",
    "\n",
    "    return alt.vconcat(stages, tasks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying available metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_metrics.select(\"Metric Name\").distinct().orderBy(\"Metric Name\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_byte_metrics = tidy_tasks(metrics).join(\n",
    "    metric_meta.withColumnRenamed(\"MetricName\", \"Metric Name\"), \n",
    "    \"Metric Name\", \n",
    "    how=\"outer\"\n",
    ").where(F.col(\"unit\") == \"bytes\").groupBy(\"Stage ID\", \"Task ID\", \"Metric Name\").sum(\"Metric Value\").withColumnRenamed(\"sum(Metric Value)\", \"Metric Value\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_shuffle_metrics = task_byte_metrics[task_byte_metrics['Metric Name'].str.contains('internal.metrics.shuffle')].sort_values('Task ID')\n",
    "shuffle_replacer = lambda match: \"Shuffle %s\" % match.group('metric')\n",
    "task_shuffle_metrics['Metric Name'] = task_shuffle_metrics['Metric Name'].str.replace(r'internal\\.metrics\\.shuffle\\.(?P<kind>read|write).(?P<metric>.*)$', shuffle_replacer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_and_task_charts(task_shuffle_metrics, \"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executor time metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_metrics = tidy_tasks(metrics).join(\n",
    "    metric_meta.withColumnRenamed(\"MetricName\", \"Metric Name\"), \n",
    "    \"Metric Name\", \n",
    "    how=\"outer\"\n",
    ").withColumn(\"Metric Value\", F.col(\"Metric Value\").cast(\"float\"))\n",
    "\n",
    "task_ms_metrics = task_metrics.where(F.col(\"unit\") == \"ms\").groupBy(\"Stage ID\", \"Task ID\", \"Metric Name\").sum(\"Metric Value\").withColumnRenamed(\"sum(Metric Value)\", \"Metric Value\")\n",
    "task_ns_metrics = task_metrics.where(F.col(\"unit\") == \"ns\").groupBy(\"Stage ID\", \"Task ID\", \"Metric Name\").sum(\"Metric Value\").withColumnRenamed(\"sum(Metric Value)\", \"Metric Value\").withColumn(\"Metric Value\", F.col(\"Metric Value\").cast(\"float\") / 1000000)\n",
    "\n",
    "task_time_metrics = task_ms_metrics.union(task_ns_metrics).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_executor_metrics = task_time_metrics[~task_time_metrics['Metric Name'].str.contains('internal.metrics.shuffle.')].sort_values('Task ID')\n",
    "\n",
    "stage_and_task_charts(task_executor_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting wall-clock vs CPU time with layered charts\n",
    "\n",
    "This gives us some sense of the relationship between CPU time and system time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cputime = task_time_metrics[task_time_metrics['Metric Name'].str.contains('executorCpuTime')]\n",
    "runtime = task_time_metrics[task_time_metrics['Metric Name'].str.contains('executorRunTime')]\n",
    "layered_stage_and_task_charts([runtime, cputime])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory and spill metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stage_and_task_charts(task_byte_metrics[task_byte_metrics['Metric Name'].str.contains('memory') | task_byte_metrics['Metric Name'].str.contains('size') | task_byte_metrics['Metric Name'].str.contains('pill')], \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_and_task_charts(task_byte_metrics, \"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Configuration information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt(df, id_vars=None, value_vars=None, var_name='variable', value_name='value'):\n",
    "    if id_vars is None:\n",
    "        id_vars = []\n",
    "    \n",
    "    if value_vars is None:\n",
    "        value_vars = [c for c in df.columns if c not in id_vars]\n",
    "    \n",
    "    return df.withColumn(\n",
    "        \"value_tuple\",\n",
    "        F.explode(\n",
    "            F.array(\n",
    "                *[\n",
    "                    F.struct(\n",
    "                        F.lit(vv).alias(var_name), \n",
    "                        F.col(\"`%s`\" % vv).alias(value_name)\n",
    "                    ) \n",
    "                    for vv in value_vars\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    ).select(*(id_vars + [F.col(\"value_tuple\")[cn].alias(cn) for cn in [var_name, value_name]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meltconfig(raw_df, event):\n",
    "    if event is not None:\n",
    "        if isinstance(event, list):\n",
    "            df = raw_df.where(F.col(\"Event\").isin(event))\n",
    "        else:\n",
    "            df = raw_df.where(F.col(\"Event\") == event)\n",
    "    else:\n",
    "        df = raw_df\n",
    "            \n",
    "    def helper(df, field):\n",
    "        return melt(df.select(field).dropna().select(\"%s.*\" % field))\n",
    "\n",
    "    return helper(df, \"Properties\").union(helper(df, \"System Properties\")).union(helper(df, \"Hadoop Properties\")).distinct()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = meltconfig(metrics, [\"SparkListenerEnvironmentUpdate\",\"SparkListenerJobStart\"])\n",
    "configs.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.Connection(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_appmeta(configs).toPandas().to_sql(\"configs\", conn, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metrics_table, task_meta_table = split_metrics(task_metrics)\n",
    "\n",
    "task_metrics_table.toPandas().to_sql('task_metrics', conn, index=False, if_exists='append')\n",
    "task_meta_table.toPandas().to_sql('task_meta', conn, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_all = task_metrics_table.join(task_meta_table, [\"Task ID\", \"Stage ID\", \"Application ID\", \"Application Name\"])\n",
    "task_all = task_all.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_columns = ['Application ID', 'Application Name', 'Attempt', 'Executor ID', 'Failed', 'Finish Time', 'Getting Result Time', 'Host', 'Index', 'Killed', 'Launch Time', 'Locality', 'Metric Name', 'Metric Value', 'Speculative', 'Stage ID', 'Task ID']\n",
    "\n",
    "index_columns = ['Application ID','Application Name','Attempt','Executor ID','Failed','Finish Time','Getting Result Time','Host','Index','Killed','Launch Time','Locality','Speculative','Stage ID','Task ID']\n",
    "\n",
    "wide_tasks = task_all[\n",
    "    project_columns\n",
    "].pivot_table(index=index_columns, columns=\"Metric Name\").reset_index()\n",
    "\n",
    "\n",
    "wide_tasks.to_sql('wide_tasks', conn, index=False, if_exists='append')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_nodes.toPandas().to_sql('plans', conn, index=False, if_exists='append')\n",
    "accumulable_nodes.toPandas().to_sql('accumulables', conn, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
