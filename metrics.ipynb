{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "metrics_file = \"metrics/application_1601392010735_0030\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = spark.read.json(metrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.select(\"App ID\", \"App Name\").dropna().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_dictify(df):\n",
    "    return [json.loads(row[0]) for row in df.selectExpr(\"to_json(*)\").collect()]\n",
    "\n",
    "def executor_info(df):\n",
    "    info = df.select(\"Executor Info\").dropna()\n",
    "    return collect_and_dictify(info)\n",
    "\n",
    "def plan_dicts(df):\n",
    "    return collect_and_dictify(df.select(\"sparkPlanInfo\").dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "MetricNode = namedtuple(\"MetricNode\", \"plan_node accumulatorId metricType name\")\n",
    "PlanInfoNode = namedtuple(\"PlanInfoNode\", \"plan_node parent nodeName simpleString\")\n",
    "\n",
    "def nextid():\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield i\n",
    "        i = i + 1\n",
    "    \n",
    "node_ctr = nextid()\n",
    "\n",
    "def plan_dicts(df):\n",
    "    return collect_and_dictify(df.select(\"sparkPlanInfo\").dropna())\n",
    "\n",
    "def flatplan(dicts, parent=-1, plan_nodes=None, metric_nodes=None):\n",
    "    if plan_nodes is None:\n",
    "        plan_nodes = list()\n",
    "        \n",
    "    if metric_nodes is None:\n",
    "        metric_nodes = list()\n",
    "    \n",
    "    for pd in dicts:\n",
    "        pid = next(node_ctr)\n",
    "        for m in pd['metrics']:\n",
    "            metric_nodes.append(MetricNode(pid, m['accumulatorId'], m['metricType'], m['name']))\n",
    "        \n",
    "        plan_nodes.append(PlanInfoNode(pid, parent, pd['nodeName'], pd['simpleString']))\n",
    "        \n",
    "        flatplan(pd['children'], pid, plan_nodes, metric_nodes)\n",
    "    \n",
    "    return(plan_nodes, metric_nodes)\n",
    "\n",
    "def plan_dfs(df):\n",
    "    pn, mn = flatplan(plan_dicts(metrics))\n",
    "    \n",
    "    pndf = spark.createDataFrame(data=pn)\n",
    "    mndf = spark.createDataFrame(data=mn)\n",
    "    \n",
    "    return (pndf, mndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_nodes, accumulable_nodes = plan_dfs(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn, mn = flatplan(plan_dicts(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.select(\"Event\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stageInfo(df):\n",
    "    \n",
    "    return collect_and_dictify(df.select(\"Stage Info\").dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stageInfo(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def accumulables(df, mcol='Task Info', idcol='Task ID'):\n",
    "    acc_cols = [F.col('Accumulable.%s' % s).alias('Metric %s' % s) for s in ['ID', 'Name', 'Value']]\n",
    "    obs = df.select(mcol).dropna().select('%s.*' % mcol)\n",
    "    cols = [F.col(elt) for elt in sorted(set(obs.columns) - set([idcol, 'Accumulables']))]\n",
    "\n",
    "    return obs.select(\n",
    "        idcol, \n",
    "        F.explode('Accumulables').alias('Accumulable'), \n",
    "        *cols\n",
    "    ).select(\n",
    "        idcol, \n",
    "        *(cols + acc_cols)\n",
    "    ).withColumnRenamed(\"Metric ID\", \"accumulatorId\")\n",
    "\n",
    "def tidy_metrics(df, mcol='Task Info', idcol='Task ID', interesting_metrics=None):\n",
    "    acc_cols = [F.col('Accumulable.%s' % s).alias('Metric %s' % s) for s in ['ID', 'Name', 'Value']]\n",
    "    obs = df.select(mcol).dropna().select('%s.*' % mcol)\n",
    "    cols = [F.col(elt) for elt in sorted(set(obs.columns) - set([idcol, 'Accumulables']))]\n",
    "    \n",
    "    if interesting_metrics is None:\n",
    "        interesting_metrics = F.col('Metric Name').isin(\n",
    "            'internal.metrics.resultSerializationTime',\n",
    "            'write time',\n",
    "            'shuffle write time',\n",
    "            'join time',\n",
    "            'GPU time',\n",
    "            'GPU decode time',\n",
    "            'fetch wait time',\n",
    "            'internal.metrics.executorCpuTime',\n",
    "            'internal.metrics.executorDeserializeTime',\n",
    "            'internal.metrics.jvmGCTime',\n",
    "            'internal.metrics.jvmGCTime'\n",
    "        )\n",
    "    elif isinstance(interesting_metrics, list):\n",
    "        interesting_metrics = F.col('Metric Name').isin(*interesting_metrics)\n",
    "    elif isinstance(interesting_metrics, str):\n",
    "        interesting_metrics = F.col('Metric Name').isin(interesting_metrics)\n",
    "    \n",
    "    return accumulables(df, mcol, idcol).where(interesting_metrics)\n",
    "\n",
    "def tidy_tasks(df):\n",
    "    return tidy_metrics(df, 'Task Info', 'Task ID', interesting_metrics=F.lit(True))\n",
    "\n",
    "def tidy_stages(df):\n",
    "    return tidy_metrics(df, 'Stage Info', 'Stage ID', interesting_metrics=F.lit(True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accumulables(metrics, 'Task Info', 'Task ID')\n",
    "[r[0] for r in acc.select('Metric Name').distinct().orderBy('Metric Name').collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetricMeta = namedtuple('MetricMeta', 'MetricName kind unit')\n",
    "\n",
    "metric_metas = [\n",
    "    MetricMeta('GPU decode time', 'time', 'ms'),\n",
    "    MetricMeta('GPU time', 'time', 'ms'),\n",
    "    MetricMeta('avg hash probe bucket list iters', 'count', 'iterations'),\n",
    "    MetricMeta('buffer time', 'time', 'ms'),\n",
    "    MetricMeta('build side size', 'size', 'bytes'),\n",
    "    MetricMeta('build time', 'time', 'ms'),\n",
    "    MetricMeta('collect batch time', 'time', 'ms'),\n",
    "    MetricMeta('concat batch time', 'time', 'ms'),\n",
    "    MetricMeta('data size', 'size', 'bytes'),\n",
    "    MetricMeta('duration', 'time', 'ms'),\n",
    "    MetricMeta('fetch wait time', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.diskBytesSpilled', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.executorCpuTime', 'time', 'ns'),\n",
    "    MetricMeta('internal.metrics.executorDeserializeCpuTime', 'time', 'ns'),\n",
    "    MetricMeta('internal.metrics.executorDeserializeTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.executorRunTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.input.bytesRead', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.input.recordsRead', 'count', 'records'),\n",
    "    MetricMeta('internal.metrics.jvmGCTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.memoryBytesSpilled', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.output.bytesWritten', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.output.recordsWritten', 'count', 'records'),\n",
    "    MetricMeta('internal.metrics.peakExecutionMemory', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.resultSerializationTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.resultSize', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.fetchWaitTime', 'time', 'ms'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.localBlocksFetched', 'count', 'blocks'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.localBytesRead', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.recordsRead', 'count', 'records'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.remoteBlocksFetched', 'count', 'blocks'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.remoteBytesRead', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.read.remoteBytesReadToDisk', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.write.bytesWritten', 'size', 'bytes'),\n",
    "    MetricMeta('internal.metrics.shuffle.write.recordsWritten', 'count', 'records'),\n",
    "    MetricMeta('internal.metrics.shuffle.write.writeTime', 'time', 'ms'),\n",
    "    MetricMeta('join output rows', 'count', 'rows'),\n",
    "    MetricMeta('join time', 'time', 'ms'),\n",
    "    MetricMeta('local blocks read', 'count', 'blocks'),\n",
    "    MetricMeta('local bytes read', 'size', 'bytes'),\n",
    "    MetricMeta('number of input columnar batches', 'count', 'batches'),\n",
    "    MetricMeta('number of input rows', 'count', 'rows'),\n",
    "    MetricMeta('number of output columnar batches', 'count', 'batches'),\n",
    "    MetricMeta('number of output rows', 'count', 'rows'),\n",
    "    MetricMeta('peak device memory', 'size', 'bytes'),\n",
    "    MetricMeta('peak memory', 'size', 'bytes'),\n",
    "    MetricMeta('records read', 'count', 'records'),\n",
    "    MetricMeta('remote blocks read', 'count', 'blocks'),\n",
    "    MetricMeta('remote bytes read', 'size', 'bytes'),\n",
    "    MetricMeta('scan time', 'time', 'ms'),\n",
    "    MetricMeta('shuffle bytes written', 'size', 'bytes'),\n",
    "    MetricMeta('shuffle records written', 'count', 'records'),\n",
    "    MetricMeta('shuffle write time', 'time', 'ms'),\n",
    "    MetricMeta('spill size', 'size', 'bytes'),\n",
    "    MetricMeta('sort time', 'time', 'ms'),\n",
    "    MetricMeta('time in aggregation build', 'time', 'ms'),\n",
    "    MetricMeta('time in batch concat', 'time', 'ms'),\n",
    "    MetricMeta('time in compute agg', 'time', 'ms'),\n",
    "    MetricMeta('total time', 'time', 'ns'),\n",
    "    MetricMeta('write time', 'time', 'ms')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_meta = spark.createDataFrame(data=metric_metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metrics = tidy_tasks(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metrics.select(\"accumulatorId\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulable_nodes.select(\"accumulatorId\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metrics.select(\"accumulatorId\").union(accumulable_nodes.select(\"accumulatorId\")).distinct().count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_metrics = tidy_stages(metrics)\n",
    "stage_metrics_outer = stage_metrics.join(accumulable_nodes, \"accumulatorId\", how=\"outer\")\n",
    "stage_metrics_outer.where(F.col(\"name\").isNull()).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_metrics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metrics_outer = task_metrics.join(accumulable_nodes, \"accumulatorId\", how=\"outer\")\n",
    "task_metrics_outer.where(F.col(\"name\").isNull()).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metrics_outer.where(F.col(\"name\").isNull()).select(\"Metric Name\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_plans = task_metrics.join(accumulable_nodes, \"accumulatorId\").join(plan_nodes, \"plan_node\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_to_plans.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_metrics.select(\"Metric Name\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metrics = tidy_tasks(metrics).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(task_metrics).mark_bar().encode(\n",
    "    x='Task ID:N',\n",
    "    y=alt.Y('sum(Metric Value):Q'),\n",
    "    color='Metric Name:N',\n",
    "    tooltip=['Metric Name', 'Metric Value', 'Task ID']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alt.Chart(task_metrics).mark_bar().encode(\n",
    "    x='Task ID:N',\n",
    "    y=alt.Y('sum(Metric Value):Q', stack=\"normalize\"),\n",
    "    color='Metric Name:N',\n",
    "    tooltip=['Metric Name', 'Metric Value', 'Task ID']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_metrics = tidy_stages(metrics).toPandas()\n",
    "alt.Chart(stage_metrics).mark_bar().encode(\n",
    "    x='Stage ID:N',\n",
    "    y='Metric Value:Q',\n",
    "    color='Metric Name:N',\n",
    "    tooltip=['Details', 'Metric Name', 'Metric Value', 'Stage ID']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_metrics = tidy_stages(metrics).toPandas()\n",
    "alt.Chart(stage_metrics).mark_bar().encode(\n",
    "    x='Stage ID:N',\n",
    "    y=alt.Y('sum(Metric Value):Q', stack=\"normalize\"),\n",
    "    color='Metric Name:N',\n",
    "    tooltip=['Details', 'Metric Name', 'Metric Value', 'Stage ID']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.select(\"System Properties\").dropna().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt(df, id_vars = None, value_vars=None, var_name='variable', value_name='value'):\n",
    "    if id_vars is None:\n",
    "        id_vars = []\n",
    "    \n",
    "    if value_vars is None:\n",
    "        value_vars = [c for c in df.columns if c not in id_vars]\n",
    "    \n",
    "    return df.withColumn(\n",
    "        \"value_tuple\",\n",
    "        F.explode(\n",
    "            F.array(\n",
    "                *[\n",
    "                    F.struct(\n",
    "                        F.lit(vv).alias(var_name), \n",
    "                        F.col(\"`%s`\" % vv).alias(value_name)\n",
    "                    ) \n",
    "                    for vv in value_vars\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    ).select(*(id_vars + [F.col(\"value_tuple\")[cn].alias(cn) for cn in [var_name, value_name]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melt(metrics.select(\"Properties\").dropna().select(\"Properties.*\")).dropna().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = metrics.where(F.col(\"Properties\").isNotNull()).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.select(\"Properties\").dropna().select(\"Properties.*\").select(\"`spark.app.id`\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = metrics.where(F.col(\"Properties\").isNotNull()).select(\"Stage Info.*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
